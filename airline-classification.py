# -*- coding: utf-8 -*-
"""airline-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vEM6hpm7y1SHEujDwexXBIlfcu-NEqXU

Made by: Tissya Malik

# PROBLEM STATEMENT

**To book the best airline which is economically convenient as well as provides great hospitality.
We all  spend so much time searching for the best airline which is cheap, ensures least delays in flights , provides traveller friendly deals and reliable service.**

# SOLUTION STATEMENT

**A sentiment analysis  about the problems of 6 major U.S. airline. Twitter data
was scraped from February of 2015 and classify to—
*positive
*.negative
*neutral tweets.**

# DATA DICTIONARY

The dataset is contains information about tweets of six major U.S airlines: Virgin America, Delta, Southwest, American, US Airways and United        
      
The data was extracted from kaggle.
Link to the dataset is https://www.kaggle.com/crowdflower/twitter-airline-sentiment.

Dataset contains 14640 rows and has following 15 Data Features:



1.tweet_id (Catagorical)

2.airline_sentiment(Catagorical)

3.airline_sentiment_confidence (Numeric)

4.negativereason (Catagorical)

5.negativereason_confidence (Numeric)

6.airline (Catagorical)

7.airline_sentiment_gold (Catagorical)

8.name (Catagorical)

9.negativereason_gold (Catagorical)

10.retweet_count (Numeric)

11.text (Catagorical)

12.tweet_coord (Catagorical)

13.tweet_created (Numeric)

14.tweet_location (Catagorical)

15.user_timezone (Catagorical)

#LET'S CODE!
"""

from google.colab import drive
drive.mount('drive')

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
import numpy as np
import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
# %matplotlib inline

nltk.download('gutenberg')
nltk.download('genesis')
nltk.download('inaugural')
nltk.download('nps_chat')
nltk.download('webtext')
nltk.download('treebank')

"""**Load the csv file available in the working or specified directory**"""

#creating datafrmae using pandas
US_airline=pd.read_csv('/content/drive/MyDrive/Intel AI readiness bootcamp 2021/twitter-airline-sentiment.csv')
US_airline.head() # airline_sentiment is the dependent variable

US_airline.shape ## 14640 rows and 15 variables

US_airline.info()

# Let's get the count of Airline
print(np.unique(US_airline['airline']))

print(US_airline['airline'].value_counts().sort_values())

"""**So we have data set of 6 airlines**

**Check for null values**
"""

US_airline.isnull().sum() #gives frequencies of NaN(null) values

"""## Columns such as tweet_coord, airline_setiment_gold and negativereason_gold has more than 90% missing.Hence we drop these columns"""

US_airline.drop(['tweet_coord'], axis = 1,inplace=True)
US_airline.drop(['airline_sentiment_gold'], axis = 1,inplace=True)
US_airline.drop(['negativereason_gold'], axis = 1,inplace=True)

#lets check after dropping columns.There are 12 columns left
US_airline.head()

US_airline.shape

US_airline.describe()

"""## As the data set has data for 6 airlines, let's get a count of positive, negative and neutral for each airline"""

airlines= ['US Airways','United','American','Southwest','Delta','Virgin America']
plt.figure(1,figsize=(10, 8))
for i in airlines:
    indices= airlines.index(i)
    plt.subplot(2,3,indices+1) # 2 rows, 3 columns, since index starts from 0, therefore it's indices+1
    new_df=US_airline[US_airline['airline']==i]
    count=new_df['airline_sentiment'].value_counts()
    Index = [1,2,3] # 1 for negative, 2 for neutral, 3 for positive
    plt.bar(Index,count, color=['red', 'blue', 'green'])
    plt.xticks(Index,['negative','neutral','positive'])
    plt.ylabel('Sentiments Count')
    plt.xlabel('Sentiments')
    plt.title('Count of Sentiments of '+i)
    # INTERCHANGE GREEN AND BLUE COLOUR

"""# INSIGHTS
1 US Airways have a least positive sentiment followed by American and United.

2.Virgin America has most balanced sentiments

## Basic Exploration in Text Mining

**Let's get a word count without writing a lambda function**
"""

US_airline['totalwords'] = [len(x.split()) for x in US_airline['text'].tolist()]
US_airline[['text','totalwords']].head()

US_airline['word_count'] = US_airline['text'].apply(lambda x: len(str(x).split(" ")))
US_airline[['text','word_count']].head()

"""**Number of Characters- including spaces**"""

US_airline['char_count'] = US_airline['text'].str.len() # this also includes spaces
US_airline[['text','char_count']].head()

"""**Average Word Length:**"""

def avg_word(sentence):
    words = sentence.split()
    return (sum(len(word) for word in words)/len(words))

US_airline['avg_word'] = US_airline['text'].apply(lambda x: avg_word(x))
US_airline[['text','avg_word']].head()

"""**Number of stop Words:**"""

from nltk.corpus import stopwords
nltk.download('stopwords')
stop = stopwords.words('english')

US_airline['stopwords'] = US_airline['text'].apply(lambda x: len([x for x in x.split() if x in stop]))
US_airline[['text','stopwords']].head()

"""**Number of special character:**"""

US_airline['hastags'] = US_airline['text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))
US_airline[['text','hastags']].head()

"""**Number of numerics:**"""

US_airline['numerics'] = US_airline['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
US_airline[['text','numerics']].head()

"""**Number of Uppercase Words:**"""

US_airline['upper'] = US_airline['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))
US_airline[['text','upper']].head()

US_airline['negativereason'].nunique()

NR_Count=dict(US_airline['negativereason'].value_counts(sort=False))
def NR_Count(Airline):
    if Airline=='All':
        a=US_airline
    else:
        a=US_airline[US_airline['airline']==Airline]
    count=dict(a['negativereason'].value_counts())
    Unique_reason=list(US_airline['negativereason'].unique())
    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']
    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})
    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])
    return Reason_frame
def plot_reason(Airline):

    a=NR_Count(Airline)
    count=a['count']
    Index = range(1,(len(a)+1))
    plt.bar(Index,count, color=['red','orange','yellow','green','blue','brown','gray','cyan','purple','pink'])
    plt.xticks(Index,a['Reasons'],rotation=90)
    plt.ylabel('Count')
    plt.xlabel('Reason')
    plt.title('Count of Reasons for '+Airline)

plot_reason('All')
plt.figure(2,figsize=(13, 13))
for i in airlines:
    indices= airlines.index(i)
    plt.subplot(2,3,indices+1)
    plt.subplots_adjust(hspace=0.9)
    plot_reason(i)

"""1. **Customer Service Issue is the main negative reason for US Airways,United,American,Southwest,Virgin America**
2. **Late Flight is the main negative reason for Delta.**
3. **Interestingly, Virgin America has the least count of negative reasons (all less than 60)**
4. **Contrastingly to Virgin America, airlines like US Airways,United,American have more than 500 negative reasons (Late flight, Customer Service Issue)**

## Pre-Processing

**Lower Case conversion:**
"""

US_airline['Tweet'] = US_airline['text'].apply(lambda x: " ".join(x.lower() for x in x.split()))
US_airline['Tweet'].head()

"""**Removal of Punctuation:**"""

US_airline['Tweet'] = US_airline['Tweet'].str.replace('[^\w\s]','')
US_airline['Tweet'].head()

"""**Removal of StopWords**"""

US_airline['Tweet'] = US_airline['Tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
US_airline['Tweet'].head()

"""**Common Words Removal**
1. **We will create a list of 10 frequently occurring words and then decide if we need to remove it or retain it**
2. **Reason is that this file has tweets related to flights.. So no point in keeping the words like name, unless we have tweets related to different category**
3. **Name of the specific airline is not removed as we would require the name of the airline for bigram analysis**
"""

freq = pd.Series(' '.join(US_airline['Tweet']).split()).value_counts()[:30]
freq

"""**Please note that we are removing the digit 2 here. This 2 might mean "to" in English.**"""

freq =['flight','2']

US_airline['Tweet'] = US_airline['Tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
US_airline['Tweet'].head()

"""**Rare Words Removal**
**>This is done as association of these less occurring words with the existing words could be a noise**
> **As it is difficult to make out if these words will have association in text analytics or not, hence to start with these words are kept in the dataset**
"""

freq = pd.Series(' '.join(US_airline['Tweet']).split()).value_counts()[-10:]
freq

"""**Stemming -refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach**"""

from nltk.stem import PorterStemmer
st = PorterStemmer()
US_airline['Tweet'][:5].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))

"""**Let's look at the overall distribution of positive, negative and neutral sentiments**"""

US_airline.airline_sentiment.value_counts().plot(kind='pie',autopct='%1.0f%%', colors=["red","orange","yellow"])

processed_features = US_airline.iloc[:, 20].values
labels = US_airline.iloc[:, 1].values

processed_features

labels

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)
processed_features = vectorizer.fit_transform(processed_features).toarray()

vectorizer

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)

y_train

"""## Random Forest Model"""

from sklearn.ensemble import RandomForestClassifier

RF_model = RandomForestClassifier(n_estimators=200, random_state=0)
RF_model.fit(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
rfcl = RandomForestClassifier()

grid_search = GridSearchCV(estimator = rfcl, param_grid = param_grid, cv = 3)
RF_model = RandomForestClassifier(n_estimators=200, random_state=0)
RF_model.fit(X_train, y_train)

param_grid = {
    'max_depth': [7],
    'max_features': [8],
    'min_samples_leaf': [5,10],
    'min_samples_split': [50,100],
    'n_estimators': [100]
}
grid_search.fit(X_train, y_train)

# Performance Matrix on train data set
from sklearn import metrics
y_train_predict = RF_model.predict(X_train)
model_score =RF_model.score(X_train, y_train)
print(model_score)
print(metrics.confusion_matrix(y_train, y_train_predict))
print(metrics.classification_report(y_train, y_train_predict))

# Performance Matrix on test data set
y_test_predict = RF_model.predict(X_test)
model_score = RF_model.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test, y_test_predict))
print(metrics.classification_report(y_test, y_test_predict))

"""## HYPER PARAMETER OPTIMIZATION"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [7],
    'max_features': [8],
    'min_samples_leaf': [5,10],
    'min_samples_split': [50,100],
    'n_estimators': [100]
}

rfcl = RandomForestClassifier()

grid_search = GridSearchCV(estimator = rfcl, param_grid = param_grid, cv = 3)

grid_search.fit(X_train, y_train)

grid_search.best_params_

best_grid = grid_search.best_estimator_

best_grid

# Performance Matrix on train data set
y_train_predict = RF_model.predict(X_train)
model_score =RF_model.score(X_train, y_train)
print(model_score)
print(metrics.confusion_matrix(y_train, y_train_predict))
print(metrics.classification_report(y_train, y_train_predict))

# Performance Matrix on test data set
y_test_predict = RF_model.predict(X_test)
model_score = RF_model.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test, y_test_predict))
print(metrics.classification_report(y_test, y_test_predict))

"""## Decision Tree Model"""

from sklearn import tree


DT_model= tree.DecisionTreeClassifier(random_state=1)
DT_model.fit(X_train, y_train)

# Performance Matrix on train data set
y_train_predict = DT_model.predict(X_train)
model_score = DT_model.score(X_train, y_train)
print(model_score)
print(metrics.confusion_matrix(y_train, y_train_predict))
print(metrics.classification_report(y_train, y_train_predict))

# Performance Matrix on test data set
y_test_predict = DT_model.predict(X_test)
model_score = DT_model.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test, y_test_predict))
print(metrics.classification_report(y_test, y_test_predict))

"""## Linear Discriminant Analysis (LDA)"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
LDA_model= LinearDiscriminantAnalysis()
LDA_model.fit(X_train, y_train)

# Performance Matrix on train data set
y_train_predict = LDA_model.predict(X_train)
model_score = LDA_model.score(X_train, y_train)
print(model_score)
print(metrics.confusion_matrix(y_train, y_train_predict))
print(metrics.classification_report(y_train, y_train_predict))

# Performance Matrix on test data set
y_test_predict = LDA_model.predict(X_test)
model_score = LDA_model.score(X_test, y_test)
print(model_score)
print(metrics.confusion_matrix(y_test, y_test_predict))
print(metrics.classification_report(y_test, y_test_predict))

"""**Out of the 3 models given above, considering the different between train and test dataset performance parameters, LDA has performed the best.**

## Conclusion

**Out of the 3 models given above, considering the difference between train and test dataset performance parameters, LDA has performed the best.**
"""